{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Liblary and generate data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gendata_poly_sin(n,d,min,max,w,sigma):\n    x = np.zeros((n,d+1))\n    x_1 = np.sort(np.random.rand(n)*(max-min)+min)\n    for i in range(1,d+1):\n        x[:,i]= x_1**i\n        \n    y = np.zeros(n)\n    \n    for i in range(n):\n        y[i] = w@x[i,:]+math.sin(x[i,1]*4)*1 +np.random.randn()*sigma\n    return x[:, 1],y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DNN(From Scratch)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class sigmoid():\n    def forward(self,h):\n        return 1/(1 + np.exp(-h))\n    \n    def backward(self,x):\n        return (1.0 - self.forward(x)) * self.forward(x)\n    \nclass relu():\n    def forward(self, h):\n        return h * (h > 0.0)\n    \n    def backward(self, x):\n        return 1.0 * (x > 0.0)\n\nclass MSE():\n    def forward(self, x, y):\n        return np.square(x-y)\n    \n    def backward(self, x, y):\n        return 2*(x- y)\n    \nclass DNN:\n\n    def __init__(self, input_size, hidden_size, output_size,acti, loss, lr=0.001):\n        \n        self.input_size=input_size\n        self.output_size=output_size\n        self.acti=acti\n        self.loss=loss\n        self.lr=lr\n        self.params = {}\n        #initialization of parameters \n        self.params['W1'] = np.random.randn(self.input_size, hidden_size) / np.sqrt(input_size)\n        self.params['b1'] = np.zeros(hidden_size) \n        self.params['W2'] = np.random.randn(hidden_size, output_size)/ np.sqrt(hidden_size)\n        self.params['b2'] = np.zeros(output_size)\n    \n        \n    def forward(self, x):\n        W1, W2 = self.params['W1'], self.params['W2']\n        b1, b2 = self.params['b1'], self.params['b2']\n        \n        self.params[\"A0\"]=x\n        #print(W1.shape, x.shape)\n        self.params[\"a1\"] = np.dot(self.params[\"A0\"], W1) + b1\n        self.params[\"z1\"] = self.acti.forward(self.params[\"a1\"])\n        self.params[\"a2\"] = np.dot(self.params[\"z1\"], W2) + b2\n        #print(W2.shape)\n        self.params[\"z2\"] = self.params[\"a2\"]\n        \n        return self.params[\"z2\"].ravel()\n    \n    def backward(self, t, output):\n        grads = {}\n        dy = self.loss.backward(x=output,y=t)\n        #print(dy)\n        grads['W2'] = np.dot(self.params[\"z1\"].T, dy).reshape(-1, 1)\n        #print(self.params[\"z2\"].shape)\n        grads['b2'] = np.sum(dy, axis=0)\n        \n        dz1 = np.dot(dy, self.params['W2'].T)\n        da1 = (self.acti.backward(self.params[\"a1\"]) * dz1)\n        grads['W1'] = np.dot(self.params[\"A0\"].T, da1)\n        #print(np.dot(self.params[\"A0\"].T, da1))\n        grads['b1'] = np.sum(da1, axis=0)\n\n        return grads\n    \n    \n    def update(self, grads):\n        for key in ('W1', 'b1', 'W2', 'b2'):\n            #print(key)\n            #print(self.params[key].shape,  grads[key].shape)\n            self.params[key] -= self.lr * grads[key]\n       \n    #predict\n    def predict(self, x):\n        output = self.forward(x)\n        return output\n    \n    #calc loss\n    def loss_(self, t, output):\n        loss = self.loss.forward(output,t)\n        return loss\n        \n            \n    #dispkay the layer structure\n    def display(self):\n        print(f\"the size of the input: (None, {self.input_size})\")\n        print(f\"hidden1: input(None, {self.input_size}),output({self.params['W1'].shape})\")\n        print(f\"hidden2: input({self.params['W1'].shape}),output({self.params['W2'].shape})\")\n        print(f\"the size of the output: (None, {self.output_size})\")\n    #for debug\n    def params(self):\n        return self.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, y_train=gendata_poly_sin(1000,6,0,10,[0,-2,1,0,0,0,0],0.5)\nx_test, y_test = gendata_poly_sin(300,6,0,10,[0,-2,1,0,0,0,0],0.9)\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n\nnetwork = DNN(input_size=1, hidden_size=100, output_size=1, acti=sigmoid(), loss=MSE())\nnetwork.display()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=[] #the history of loss\niters_num = 100 # the number of the epoch\nbestloss=np.inf # recording the lowest loss at the time\nfor i in range(iters_num):\n    train_byepoch_list = []\n    for x, y in zip(x_train,y_train):\n        output = network.forward(x)#forward\n        grads = network.backward(y, output)#backward\n        network.update(grads)#update paramters\n        loss = network.loss_(y, output)# calc loss\n        train_byepoch_list.append(loss)\n        \n        if loss < bestloss:\n            bestloss=loss\n            \n    history.append(np.mean(train_byepoch_list))\n    \n    if i % 10 ==0:# display log\n        print(\"epochs:\", i, \"======== loss:\", np.mean(train_byepoch_list))  \nprint(\"The best train loss: \",bestloss) \n\n#predict\npred=[]\nfor test in zip(x_test):\n    pred.append(network.predict(test))\n\nprint(\"Test MSE:\", network.loss_(y_test, np.array(pred)).mean())\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(x_train,y_train, label=\"train\")\nplt.plot(x_test,y_test, label=\"test\")\nplt.plot(x_test, pred, label=\"pred\")\nplt.legend(bbox_to_anchor=(1, 1), loc='Upper Left', borderaxespad=0, fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DNN(Keras)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.optimizers import SGD, Adam\nfrom keras.optimizers import Optimizer\nfrom keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SGD(not work well)\nmodel = keras.Sequential()\nmodel.add(layers.Dense(100, input_dim=1, activation='relu', kernel_initializer='random_normal',bias_initializer='zeros'))\nmodel.add(layers.Dense(1, kernel_initializer='random_normal',bias_initializer='zeros'))\nmodel.compile(optimizer=SGD(momentum=0.0, learning_rate=0.001), loss='mean_squared_error')\nmodel.summary()\n\nbatch_size = 1\nepochs = 100\n\nkeras_history=model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\nprint(\"Learning done!\")\n\nfrom sklearn.metrics import mean_squared_error\npred_keras=model.predict(x_test)\nprint(\"Test MSE: \", mean_squared_error(y_test, pred_keras))\n\nplt.plot(x_train,y_train, label=\"train\")\nplt.plot(x_test,y_test, label=\"test\")\nplt.plot(x_test, pred_keras, label=\"pred\")\nplt.legend(bbox_to_anchor=(1, 1), loc='Upper Left', borderaxespad=0, fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adam version\nmodel = keras.Sequential()\nmodel.add(layers.Dense(100, input_dim=1, activation='relu', kernel_initializer='random_normal',bias_initializer='zeros'))\nmodel.add(layers.Dense(1, kernel_initializer='random_normal',bias_initializer='zeros'))\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\nmodel.summary()\n\nbatch_size = 1\nepochs = 100\n\nkeras_history=model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\nprint(\"Learning done!\")\n\nfrom sklearn.metrics import mean_squared_error\npred_keras=model.predict(x_test)\nprint(\"Test MSE: \", mean_squared_error(y_test, pred_keras))\n\nplt.plot(x_train,y_train, label=\"train\")\nplt.plot(x_test,y_test, label=\"test\")\nplt.plot(x_test, pred_keras, label=\"pred\")\nplt.legend(bbox_to_anchor=(1, 1), loc='Upper Left', borderaxespad=0, fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}